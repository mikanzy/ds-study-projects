# Определение токсичных комментариев с использованием BERT

## Описание

Попытка построить модель классификации комментариев на позитивные (нормальные) и негативные (токсичные) с использованием модели BERT для извлечения признаков (feature extraction) из текстов. Построенная модель не достигла целевого уровня качества.


## Данные

Набор данных с разметкой о токсичности комментариев:

- Текст комментария;
- Целевой признак: 0 для нормальных комментариев и 1 для токсичных.


## Цель проекта

Построить модель классификации комментариев на позитивные (нормальные) и негативные (токсичные) с использованием модели BERT для извлечения признаков (feature extraction) из текстов.

Значение метрики качества *F1* должно быть не меньше 0.75. 


## Задачи проекта

1. Загрузить и подготовить данные.

2. Обучить разные модели классификации на данных.

3. Проанализировать результаты и сделать выводы.


## Используемые библиотеки

*catboost, langid, matplotlib, numpy, pandas, scikit-learn, seaborn, shap, torch, tqdm, transformers*


## Основные результаты

1. Установлено, что все тексты в корпусе на **английском языке**.

2. Проведена **обработка** текстов:
    - приведение символов к нижнему регистру,
    - очистка от URL, email, HTML-тегов, IP-адресов, экранированных специальных символов,
    - удаление пустых текстов, появившихся в результате обработки.

3. С помощью предобученной модели **BERT** (вариант `bert-base-uncased`) для каждого текста получен эмбеддинг CLS-токена — вектор размерностью 768. Эти эмбеддинги использованы в качестве набора признаков для обучения ML модели.

4. К данным для обучения ML моделей добавлен **дополнительный признак** — длина текста в словах.
  
5. Обнаружен **дисбаланс классов**: доля токсичных комментариев в датасете составляет всего 10%. Этот факт учтён и при подготовке выборок для обучения и проверки качества ML модели, и при построении модели.

6. Построена и обучена **ML модель**, которая классифицирует комментарии на токсичные и нормальные. Модель в принципе адекватна, но по качеству не дотягивает до заданного уровня: значение метрики *F1* модели на тестовой выборке получилось 0.68. 

7. Построенная ML модель смогла верно выявить только **65%** токсичных комментариев, при этом среди комментариев, которые модель классифицировала как токсичные, таковыми на самом деле оказались **73%**. 


## Заключение

Построенная модель классификации не удовлетворяет требованиям заказчика по критерию метрики *F1*: модель недостаточно хорошо выявляет токсичные комментарии, а также нередко считает токсичными и нормальные комментарии. Таким образом, ванильная модель BERT не справляется с задачей извлечения из текстов признаков, которые позволили бы качественно классифицировать комментарии на токсичные и нетоксичные.

**Рекомендации по повышению качества модели классификации:**
- Дообучить модель BERT на данных из датасета с комментариями (т.е. провести fine-tuning модели). Однако обучение нейросети с большим количеством параметров требует очень больших вычислительных ресурсов.

- Использовать для извлечения признаков уже готовый, дообученный соответствующим образом, вариант модели BERT, либо другую языковую модель, которых существует множество. Например, модель RoBERTa, которая должна показать лучшие результаты, чем BERT.

- Использовать для модели классификации признаки на основе значений TF-IDF для отдельных слов в тесте комментариев, что даже проще с точки зрения требуемых вычислительных ресурсов.